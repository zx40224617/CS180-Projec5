<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <title>Project 4</title>
  </head>

  <body class="pageContents">
    <h1>Project 5 - Part A: The Power of Diffusion model</h1>
    <div class="wrapper">
      <h2 class="title">Background of the project:</h2>
      <p class="introduction">
        In this part of the project, we are going to do a lot of fun things with
        the pre-trained diffusion model from DeepFloyd including denoising,
        inpainting, creating visual anagram and hybrid image.
      </p>
      <h3>Part 0. Setup</h3>
      <p class="description">
        This part is just for us to see what results will the diffusion model
        return. We first need to follow the steps in the project spec to gain
        access to use the diffustion model. And then we use it to generate some
        image base on the prompt. Note that the prompt here are text embeding of
        a text instead of a real text. And we need to set a seed for this part
        and all the later parts of the project, I am using <b>180</b>. Here are
        the rsults with different prompts and inference steps.
      </p>
      <p class="imgTitle">inference steps = 20:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of a snowy mountain village:
          </p>
          <img src="images/snowy_mountain_20.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a man wearing a hat:</p>
          <img src="images/mean_wearing_hat_20.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a rocket ship:</p>
          <img src="images/rocket_ship_20.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">inference steps = 50:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of a snowy mountain village:
          </p>
          <img src="images/snowy_mountain_50.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a man wearing a hat:</p>
          <img src="images/man_wearing_hat_50.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a rocket ship:</p>
          <img src="images/rocket_ship_50.png" class="resultImage" />
        </div>
      </div>
      <hr />
      <h3>Part 1 - 1. Implementing the Forward Process</h3>
      <p class="description">
        For this part, we are using the pre-trained denoiser of DeepFloyd. But
        first need to implement the forward pass so that we can add noise to the
        images. Typically, the clean image is \(x_0\) and \(x_T\) is pure noise.
        Which mean larger t has more noise, and for DeepFloyd models, T = 1000.
        So in this forward implementation, we need to add the noise to the image
        base on the t that is given. Which follows this formula:
        \(\sqrt{\bar{\alpha_t}} x_0 + \sqrt{1 - \bar{\alpha_t}} \epsilon\) where
        \(\epsilon\) ~ \(N (0, 1)\). Note that we did not just add the noise,
        but also scale the image. The \(\bar{\alpha}\) is called the
        <b>alphas_cumprod</b> variable. Which contains the \(\bar{\alpha_t}\)
        for \( t \in [0, 999] \). And we can get it by calling
        <b>stage_1.scheduler.alphas_cumprod</b>. Here are my results:
      </p>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/original_camp.png" class="resultImage" />
      </div>
      <p class="imgTitle">Images after adding noise:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 250</p>
          <img src="images/camp_t = 250.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 500:</p>
          <img src="images/camp_t = 500.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 750:</p>
          <img src="images/camp_t = 750.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        As you can see, the image is more noisy when t is higher.
      </p>
      <hr />
      <h3>Part 1 - 2. Classical Denoising</h3>
      <p class="description">
        This part is farily simple, before we start to use the diffustion model
        for denoising, let's try the classical denoising method -
        <b>Gaussian blur filtering</b>. Here are my results:
      </p>
      <p class="imgTitle">Images Before Gaussian blur filtering:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 250</p>
          <img src="images/camp_t = 250.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 500:</p>
          <img src="images/camp_t = 500.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 750:</p>
          <img src="images/camp_t = 750.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Images After Gaussian blur filtering:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 250</p>
          <img src="images/blur_camp_250.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 500:</p>
          <img src="images/blur_camp_500.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 750:</p>
          <img src="images/blur_camp_750.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        You can tell that the results is really bad, and that's why we need the
        diffusion model for the job.
      </p>
      <hr />
      <h3>Part 1 - 3. One-Step Denoising</h3>
      <p class="description">
        In this part, we can finally start using the pre-trained diffusion model
        to do the denoising job. In this part, we are implementing one-step
        denosing. Which means that given a noisy image \(x_t\) and the timestep
        t, we predict the noise to directly obtain \(x_0\), which is the clean
        image. Note that since the model is trained with text conditionding, we
        also need to pass in a text prompt embedding, which we use
        <b>"a high quality photo"</b> here. Here are my results:
      </p>
      <p class="imgTitle">Images Before One-Step Denoising:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 250</p>
          <img src="images/camp_t = 250.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 500:</p>
          <img src="images/camp_t = 500.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 750:</p>
          <img src="images/camp_t = 750.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Images After One-Step Denoising:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 250</p>
          <img src="images/one_step_camp_250.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 500:</p>
          <img src="images/one_step_camp_500.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 750:</p>
          <img src="images/one_step_camp_750.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        As you can see, the predicted results of higher t (more noisy image)
        will be more depart from the original image.
      </p>
      <p>
        <b class="note">Note:</b>
        Since we add noise to the image using the formula:
        \(\sqrt{\bar{\alpha_t}} x_0 + \sqrt{1 - \bar{\alpha_t}} \epsilon\) where
        \(\epsilon\) ~ \(N (0, 1)\). When we obtain the denoised image using the
        perdiction of the noise, we need to derive tge formula to obtain
        \(x_0\).
      </p>
      <hr />
      <h3>Part 1 - 4. Iterative Denoising</h3>
      <p class="description">
        Since diffusion models are designed to denoise iteratively. In this
        part, we are implementing iterative denoising. Which means that at every
        timestep, we denoise and obtain the image at previous timestep. like we
        can start with \(x_{1000}\), and get \(x_{999}\), and then \(x_{998}\),
        and keep continue until we get \(x_0\). But it will take a lot of time
        and computing poer if we run the diffusion model that many times. We can
        actually skip some steps. By using a
        <b>strided_timesteps</b> (an arrray of timesteps where
        strided_timesteps[0] is the largest t (990 in this case), and
        strided_timesteps[-1] is 0), we can predicted the image at
        strided_timesteps[i + 1] when we are at strided_timesteps[i]. For
        example, in this implementation, my stride is 30. Let's say I start at
        \(x_{990}\), I will then get \(x_{960}\), and then \(x_{930}\) until I
        get \(x_0\). Which save a lot of time and computing power but still
        works fine. For every iteration, we will need to perdict the image at
        previosu time using the following formula and constants:
      </p>

      <img
        src="images/Screenshot 2024-11-20 at 12.25.37â€¯AM.png"
        class="noteImage"
      />
      <p class="description">
        where t is strided_timesteps[i], and t' is just strided_timesteps[i+1].
        And the way to get \(x_0\) is the same as how we got the clean image in
        one-step denoising. Here are my results:
      </p>
      <p class="imgTitle">Iterative Denoising Images:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 690</p>
          <img src="images/iterative_camp_690.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 540:</p>
          <img src="images/iterative_camp_540.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 390:</p>
          <img src="images/iterative_camp_390.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Iterative Denoising Images:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 240</p>
          <img src="images/iterative_camp_240.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 90:</p>
          <img src="images/iterative_camp_90.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">t = 0:</p>
          <img src="images/iterative_camp_final.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Result Comparison:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Iterative Denoising</p>
          <img src="images/iterative_camp_final.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">One-Step Denoising:</p>
          <img src="images/one_step_camp_final.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Gaussian blur filtering</p>
          <img src="images/gaussian_camp_final.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        It is quite obvious that the iterative denoising method gives us the
        best result compare to one-step amd Gaussian blur.
      </p>
      <p>
        <b class="note">Note:</b>
        Note that the image timestep start at 690 here, it is because we start
        at strided_timesteps[10] (i_start = 10). This is because that we want to
        at least give model some information about the campenelle so that it can
        give us some results that still look like a campenelle.
      </p>
      <hr />
      <h3>Part 1 - 5. Diffusion Model Sampling</h3>
      <p class="description">
        Like what we discussed at the end of the previous part, we use i_start =
        10 to perserve some information of the original image so that the
        diffusion model will output some result that looks similar to the
        original image. Which means when we use i_start = 0, and pass in a pure
        noise, we are basically denoising the pure noise, which we can genrate
        image from scratch. Here are my results:
      </p>
      <p class="imgTitle">Sample Images:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Sample 1;</p>
          <img src="images/sample1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Sample 2:</p>
          <img src="images/sample2.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Sample 3:</p>
          <img src="images/sample3.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Sample 4:</p>
          <img src="images/sample4.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Sample 5:</p>
          <img src="images/sample5.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        As we can see, the results here are actually a little blury and the
        quality is bad, some of them are just non-sense. Which we will fix that
        in the next section.
      </p>
      <hr />
      <h3>Part 1 - 6. Classifier-Free Guidance (CFG)</h3>
      <p class="description">
        As we see in the previous part, the result are bad in quality. So in
        order to imporve out results, we can implement classifier-free guidance
        (CFG). For CFG, we first compute both conditional and unconditional
        noise estimate, which is \(\epsilon_{c}\) and \(\epsilon_{u}\). And then
        we get a new noise estimation following the formula: \(\epsilon
        =\epsilon_{u} + \gamma(\epsilon_{c} - \epsilon_{u})\). Where \(\gamma\)
        denote the strength of the CFG, and for a good quality image, we need
        \(\gamma\) > 1 (I use \(\gamma\) = 7 here). Note that the unconditional
        genration prompt is <b>"a high quality photo"</b> and unconditional
        guidance prompt is <b>""</b>, which is the null prompt. Here are my
        results:
      </p>
      <p class="imgTitle">Sample Images:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">CFG Sample 1;</p>
          <img src="images/CFG_sample1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">CFG Sample 2:</p>
          <img src="images/CFG_sample2.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">CFG Sample 3:</p>
          <img src="images/CFG_sample3.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">CFG Sample 4:</p>
          <img src="images/CFG_sample4.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">CFG Sample 5:</p>
          <img src="images/CFG_sample5.png" class="resultImage" />
        </div>
      </div>
      <p>
        <b class="suggestion">Observation:</b>
        As we can see, the results here are a lot better in quality than the
        previous part without CFG. And there are no non-sensical images anymore.
      </p>
      <hr />
      <h3>Part 1 - 7. Image-to-image Translation</h3>
      <p class="description">
        As we see in part 1 - 4, when we denoise an image with noise, the result
        it predicted is "edited". And with more noise, the editing effects is
        stronger. This is pretty normal since it need the model to "hallucinate"
        a little bit for it to denoise the image onto the manifold of natural
        images. So in this part, by starting at different i_start (different
        noise level), we can see different levels of editing. And when we start
        at a really low noise level, the results is going to be really similar
        to the original image, which is the SDEdit algorithm. Here are my
        results:
      </p>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/original_camp.png" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/camp_SDE_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/camp_SDE_2.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/camp_SDE_3.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/camp_SDE_4.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/camp_SDE_5.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/camp_SDE_6.png" class="resultImage" />
        </div>
      </div>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/Doe.jpeg" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/Doe_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/Doe_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/Doe_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/Doe_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/Doe_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/Doe_20.png" class="resultImage" />
        </div>
      </div>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/CMM.jpeg" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/CMM_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/CMM_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/CMM_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/CMM_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/CMM_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/CMM_20.png" class="resultImage" />
        </div>
      </div>
      <hr />
      <h3>Part 1 - 7 - 1. Editing Hand-Drawn and Web Images</h3>
      <p class="description">
        We can also try to start with some unrelistic image and project it onto
        the natural image manifold using the same techniques that we use in the
        previous part. Here are my results:
      </p>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/suisei.jpeg" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/suisei_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/suisei_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/suisei_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/suisei_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/suisei_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/suisei_20.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/flower.png" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/flower_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/flower_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/flower_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/flower_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/flower_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/flower_20.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/sword.png" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with different noise level:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/sword_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/sword_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/sword_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/sword_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/sword_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/sword_20.png" class="resultImage" />
        </div>
      </div>
      <h3>Part 1 - 7 - 2. Inpainting</h3>
      <p class="description">
        We can also try to implement the same procedure to do inpainting. Which
        given an image and a binary mask, we left out everything outside the
        mask the same, but the thing inside the mask is generated by the
        diffusion model. Which is a really similar precedure as the previous
        part, but using the formula \(x_t = mx_t + (1 - m)forward(x_{origin,
        t})\), here are my results:
      </p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image:</p>
          <img src="images/original_camp.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Mask:</p>
          <img src="images/camp_mask.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Inpainted Image:</p>
          <img src="images/inpainted_camp.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image:</p>
          <img src="images/table_tennis.jpeg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Mask:</p>
          <img src="images/table_tennis_mask.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Inpainted Image:</p>
          <img src="images/inpainted_table_tennis.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Original Image:</p>
          <img src="images/coffee.jpeg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Mask:</p>
          <img src="images/coffee_mask.jpg" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">Inpainted Image:</p>
          <img src="images/inpainted_coffee.png" class="resultImage" />
        </div>
      </div>
      <h3>Part 1 - 7 - 3. Text-Conditional Image-to-image Translation</h3>
      <p class="description">
        For this part, we are doing the samething as normal SDEdit, but thus
        time, instead of <b>"a high quality photo"</b>, we are using other
        prompt to guide projection.
      </p>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/original_camp.png" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with prompt: "a rocket ship":</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/rocket_camp_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/rocket_camp_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/rocket_camp_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/rocket_camp_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/rocket_camp_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/rocket_camp_20.png" class="resultImage" />
        </div>
      </div>
      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/table_tennis.jpeg" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with prompt: "a pencil":</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/table_tennis_pencil_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/table_tennis_pencil_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/table_tennis_pencil_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/table_tennis_pencil_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/table_tennis_pencil_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/table_tennis_pencil_20.png" class="resultImage" />
        </div>
      </div>

      <p class="imgTitle">Original Image:</p>
      <div class="imageWrapper">
        <img src="images/coffee.jpeg" class="resultImage" />
      </div>
      <p class="imgTitle">SDEdit with prompt: "a photo of a dog":</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 1;</p>
          <img src="images/coffee_dog_1.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 3:</p>
          <img src="images/coffee_dog_3.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 5:</p>
          <img src="images/coffee_dog_5.png" class="resultImage" />
        </div>
      </div>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 7:</p>
          <img src="images/coffee_dog_7.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 10:</p>
          <img src="images/coffee_dog_10.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">i_start = 20:</p>
          <img src="images/coffee_dog_20.png" class="resultImage" />
        </div>
      </div>
      <hr />
      <h3>Part 1 - 8. Visual Anagrams</h3>
      <p class="description">
        In this part, we are using the diffusion model to create a visual
        Anagram. Which will look like an image right-side up, and looks like
        another image upside down. Which we will need 2 prompts in total. The
        idea is actually really easy, we get the conditional and unconditonal
        noise perdiction of the first prompt and use CFG to get noise1, and then
        we flip the image and get the conditonal and unconditonal noise
        perdiction of the fliped image as well. And we use CFG to get noise2. We
        then flip the noise2 back to right-side up, and average noise1 and
        noise2 to get the final noise, the rest will be the same procedure as
        what we did in the ' previous parts. Here are my results:
      </p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">an oil painting of an old man:</p>
          <img src="images/Oldman.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of people around a campfire:
          </p>
          <img src="images/CampFire.png" class="resultImage" />
        </div>
      </div>

      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a rocket ship:</p>
          <img src="images/rocket_pencil.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a pencil:</p>
          <img src="images/pencil_rocket.png" class="resultImage" />
        </div>
      </div>

      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of a snowy mountain village:
          </p>
          <img src="images/snowy_cost.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a photo of the amalfi cost:</p>
          <img src="images/cost_snowy.png" class="resultImage" />
        </div>
      </div>
      <hr />
      <h3>Part 1 - 9. Hybrid Images</h3>
      <p class="description">
        For this part, we are doing hybrid image, which the image will look like
        1 thing viewing from close up, and another thing viewing from far away.
        The idea is really similar to the previous part. At first, we get the
        conditonal and unconditonal noise prediction for both prompts (no
        flipping), and get 2 CFG as the previous part. After we got 2 CFG, we
        use guassian blur to get the low frequency of one of the image and high
        frequency of the other image. And just add the 2 frequency together (no
        averaging) Here are my results:
      </p>
      <p class="imgTitle">Hybrid Image of skull and waterfall:</p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a lithograph of waterfalls:</p>
          <img src="images/skull_waterfall.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a lithograph of a skull:</p>
          <img src="images/skull_waterfall.png" class="hybrid-image-small" />
        </div>
      </div>

      <p class="imgTitle">
        Hybrid Image of old man and snowy mountain village:
      </p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of a snowy mountain village:
          </p>
          <img src="images/snowy_old_man.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">an oil painting of an old man:</p>
          <img src="images/snowy_old_man.png" class="hybrid-image-small" />
        </div>
      </div>

      <p class="imgTitle">
        Hybrid Image of snowy mountain village and waterfall:
      </p>
      <div class="imageWrapper">
        <div class="title-image-wrapper">
          <p class="imagSubTitle">a lithograph of waterfalls:</p>
          <img src="images/snowy_waterfall.png" class="resultImage" />
        </div>
        <div class="title-image-wrapper">
          <p class="imagSubTitle">
            an oil painting of a snowy mountain village:
          </p>
          <img src="images/snowy_waterfall.png" class="hybrid-image-small" />
        </div>
      </div>
    </div>

    <h1>Project 5 - Part B: Diffusion Models from Scratch!</h1>
    <div class="wrapper">
      <h2 class="title">Background of the project:</h2>
      <p class="introduction">
        We already see how and what the pre-trained diffusion model can achieve
        in the previous part, so we now can try to implement and train out own
        diffusion model. In this part, we are trainning the diffusion model
        using the MNIST hand-written digits dataset so that we can denoise a
        hand-written digits with noise back to the pure image of digits.
      </p>
      <h3>Part 1. Training a Single-Step Denoising UNet</h3>
      <h4>Part 1 - 1. Implementing the UNet:</h4>
      <p class="description">
        For this part, we are tainning an easy one-step denoiser that given a
        noisy image z, we predict the original clean image x by optimize over
        the following loss function: \(\mathbf{L} = \mathbf{E}_{z,
        x}\|\mathbf{D}_{\theta}(z) - x\|^2\). And we are implementing a network
        structure called Unet for the denoiser. And the main idea for the Unet
        is that it has an encoder where it downsample the image, and the decoder
        which upsample the encoded image. Also, to perserve the detaisl of the
        images, we will use what called skip-connections. The details are all in
        the project spec, which is easy to follow.
      </p>
      <h4>Part 1 - 2. Using the UNet to Train a Denoiser:</h4>
      <p class="description">
        Before we start traing, we need to create the dataset first. Here, we
        create the noisy images using the formula : \(z = x + \sigma\epsilon\)
        where \(\epsilon\) ~ \(N(0, 1)\). which the clean image is the x in the
        loss funtion and z is the noisy image. Here are my results:
      </p>
      <img class="noteImage" src="images/add_noise.png" />
      <p class="description">
        After we got the image sets, we can start trianing right now. Note that
        for training, we use the noisy images with \(\sigma = 0.5\). All the
        recommened setup and hyper parameters are given in the set, here are my
        results:
      </p>
      <p class="imgTitle">Training Loss:</p>
      <img class="noteImage" src="images/unconditional_training_loss.png" />
      <p class="imgTitle">Result at epoch 1:</p>
      <img class="noteImage" src="images/unconditional smapling e1.png" />
      <p class="imgTitle">Result at epoch 5:</p>
      <img class="noteImage" src="images/unconditional sampling e5.png" />
      <p class="description">
        Since we trained the networks using \(\sigma = 0.5\), let's see how the
        results will be if we usedifferent noise levels, here are my results:
      </p>

      <img class="noteImage" src="images/all_level_noise_output_7.png" />
      <img class="noteImage" src="images/all_level_noise_output_2.png" />
      <img class="noteImage" src="images/all_level_noise_output_1.png" />
      <p>
        <b class="suggestion">Observation:</b>
        As we can see, with higher noisy level (higher \(\sigma\)), the denoised
        results is worse, which makes sense since it is harder for the model to
        denoise with more noise in the image.
      </p>
      <hr />
      <h3>Part 2. Training a Diffusion Model</h3>
      <p class="description">
        Just like in part A, after we finished one-step denoising, we can now
        implement iterative denoising. Although we are still using MSE loss as
        in the previous part, but this time, instead of making the model perdict
        the clean image, we want it to perdict the noise, so the loss function
        is as following: \(\mathbf{L} = \mathbf{E}_{\epsilon,
        x}\|\epsilon_{\theta}(z) - \epsilon\|^2\). Like what we did in part A
        for iterative denosing, \(x_t = \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1 -
        \bar{\alpha_t}} \epsilon\), so we need all the constants, but this time
        , we are creating the constans ourselves. And of course, since we are
        doing iterative denoising, we need to take tinestep as an input to the
        model as well, so the final loss function will become: \(\mathbf{L} =
        \mathbf{E}_{\epsilon, x_0, t}\|\epsilon_{\theta}(x_t, t) -
        \epsilon\|^2\).
      </p>
      <h4>Part 2 - 1. Adding Time Conditioning to UNet</h4>
      <p class="description">
        In order to add our time condition into the network, we need a new Block
        called FCB block. Note that since the t we pass in is a bacth of
        timesteps, we need to braodcast it to something that matches the shape
        of the results of upblocks. And since each t is a scalar, we need to
        normalize t to be in the range of [0, 1] here.
      </p>

      <h4>Part 2 - 2. Training the UNet</h4>
      <p class="description">
        Now, we can finally start training our time-conditioned Unet. The
        detailed algorithm is in the project spec, but note that for every
        image, we pick a random timestep, and don't forget to normalize t. And
        we also add a learing rate scheduler here to update learning rate after
        each epoch. Here are my results using the recommened hyper parameters:
      </p>

      <img class="noteImage" src="images/time_conditioned_training_loss.png" />

      <h4>Part 2 - 3. Sampling from the UNet</h4>
      <p class="description">
        The sampling here is technically the same as what we did in part A, but
        over here, we don't need to predict teh varaince. we can instead use the
        list \(\beta\) that we created. Here are my results:
      </p>
      <p class="imgTitle">Result at epoch 1:</p>
      <img class="noteImage" src="images/time_conditioned_sampling_e1.png" />
      <p class="imgTitle">Result at epoch 5:</p>
      <img class="noteImage" src="images/time_conditioned_sampling_e5.png" />
      <p class="description"></p>
      <p class="imgTitle">Result at epoch 10:</p>
      <img class="noteImage" src="images/time_conditioned_sampling_e10.png" />
      <p class="imgTitle">Result at epoch 15:</p>
      <img class="noteImage" src="images/time_conditioned_sampling_e15.png" />
      <p class="description"></p>
      <p class="imgTitle">Result at epoch 20:</p>
      <img class="noteImage" src="images/time_conditioned_sampling_e20.png" />

      <p>
        <b class="suggestion">Observation:</b>
        As we can see, the results are getting better as the epoch number
        increases, but the final result are still somewhat noisy and bad. So we
        will implement class conditioned in the next section to get a even
        better result.
      </p>

      <hr />
      <h4>Part 2 - 4. Adding Class-Conditioning to UNet</h4>
      <p class="description">
        As we see in the previous section, the results are not good enough, so
        we need to add class condition (labels 0 - 9) for better results and
        image genration. In order to do this, we need 2 more FCB blocks for
        class. Note that we want the class labels to be one_hot_encoded. Also,
        since we still want the model to work without class conditioning, we
        implement a drop out mask that will drop 10% of the class conditioning
        vector to a zero vector. Here are my results using the recommened hyper
        parameters:
      </p>
      <img class="noteImage" src="images/class_conditioned_training_loss.png" />

      <h4>Part 2 - 5. Sampling from the Class-Conditioned UNet</h4>
      <p class="description">
        As what we observed in part A, we need to do CFG in order to get a good
        result. Note that for unconditional predictin, we just pass in a mask
        with 100% drop out rate. Here are my results:
      </p>

      <p class="imgTitle">Result at epoch 1:</p>
      <img class="noteImage" src="images/class_conditioned_sampling_e1.png" />
      <p class="imgTitle">Result at epoch 5:</p>
      <img class="noteImage" src="images/class_conditioned_sampling_e5.png" />
      <p class="description"></p>
      <p class="imgTitle">Result at epoch 10:</p>
      <img class="noteImage" src="images/class_conditioned_sampling_e10.png" />
      <p class="imgTitle">Result at epoch 15:</p>
      <img class="noteImage" src="images/class_conditioned_sampling_e15.png" />
      <p class="description"></p>
      <p class="imgTitle">Result at epoch 20:</p>
      <img class="noteImage" src="images/class_conditioned_sampling_e20.png" />
      <p>
        <b class="suggestion">Observation:</b>
        As we can see, the results are so much better than the previous part,
        even early epoch gives a really good result.
      </p>
      <hr />
      <h3>Final Reflection of the Project</h3>
      <p class="description">
        Althouhg this project is really long and hard, I still think it is
        really intersting to see how powerful the machine learning model are in
        computer vision. Which gives me more idea of what I can do with computer
        vision right now with all the different models. For no matter who is
        reading this text, this is a really great semester, thank you for your
        help.
      </p>
    </div>
  </body>
</html>
